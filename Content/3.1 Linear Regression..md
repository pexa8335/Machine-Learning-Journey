---
tags:
  - ML
  - "#LinearRegression"
Date: 2025-09-16
Video: https://www.youtube.com/watch?v=3dC-_GAs2zI&list=PLaKukjQCR56ZRh2cAkweftiZCF2sTg11_&index=7
Relevant: "[[1.1 & 1.2 Learning problem of machine.]]"
---
This is a sub-problem of **Supervised learning** discussed in [[1.1 & 1.2 Learning problem of machine.]].

# Regression problem.

Learn a function $y = f(x)$ from a given training set {${x_{1}, x_{2}, \dots x_{n}}$} ; {$y_{1}, y_{2}, \dots{y_{m}}$} so that $y_{i} \cong f(x_{i})$.

Each **observation** of $x$ is represented by **an** n-dimensional vector in **an** n-dimensional space.

For instance, a vector $x$ can be represented as **an** n-**element** vector $x_{i} = (x_{i_{1}}, x_{i_{2}}, \dots x_{i_{n}})^T$.

>[!note]
>$x^T$ is a **transposed** matrix. Normally, vectors are represented **horizontally**, so to represent it as a column **vector**, we use $x^T$.

---
## Linear model.

If $f(x)$ is assumed to be of linear form:

$$
f(x) = w_{0} + w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}
$$
	$w_{0}, w_{1}, \dots, w_{n}$ are the regression coefficients/weights.
	$w_{0}$ is sometimes called **bias**.

For each observation $x$, we have a corresponding **weight** $w$. If an observation **has a** higher $w$, it **indicates** that this vector $x$ (or feature $x$) is important.

In the linear function $f(x) = w_{0} + w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}$:

*   **$w_0$ is called the intercept term or bias term.**
*   It represents the value of $f(x)$ when all input variables $x_1, x_2, \dots, x_n$ are equal to 0.

Let's consider two cases:

1.  **When $w_0$ is present:**
    If you set all $x_i = 0$, we get:
    $f(0) = w_{0} + w_{1}(0) + w_{2}(0) +\dots + w_{n}(0)$
    $f(0) = w_{0}$

    This means that when all inputs are 0, the value of the function is $w_0$. If $w_0 \neq 0$, then the function will not pass through the origin $(0, 0, \dots, 0)$.

2.  **When $w_0$ is removed (i.e., $w_0 = 0$):**
    The function becomes:
    $f(x) = w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}$

    Now, if you set all $x_i = 0$, we get:
    $f(0) = w_{1}(0)+w_{2}(0) +\dots + w_{n}(0)$
    $f(0) = 0$

    **Therefore, when $w_0$ is removed (or set to 0), the function $f(x)$ will always pass through the origin $(0, 0, \dots, 0)$** because when all inputs are 0, the output is also 0.

In a geometric context, $w_0$ determines the point where the line (or plane, hyperplane) intersects the $f(x)$ axis (or y-axis in 2D space) when all independent variables are zero. If $w_0=0$, that intersection point is precisely the origin.

>[!note]
>If a function can only pass through origin (0, 0,..., 0) it may not perform well so we add $w_{0}$.

>[!tip] Important
>Learning a linear model is equivalent to learning the coefficient vector $w = (w_{0}+ w_{1} + \dots + w_{n})^T$.

![[Pasted image 20250916124102.png]]

In **Linear Regression**, our missionn is to find the best line to represent all the data points.