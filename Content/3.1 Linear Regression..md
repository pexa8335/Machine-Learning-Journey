---
tags:
  - ML
  - "#LinearRegression"
Date: 2025-09-16
Video: https://www.youtube.com/watch?v=3dC-_GAs2zI&list=PLaKukjQCR56ZRh2cAkweftiZCF2sTg11_&index=7
Relevant: "[[1.1 & 1.2 Learning problem of machine.]]"
---
This is a sub-problem of **Supervised learning** discussed in [[1.1 & 1.2 Learning problem of machine.]].

# Regression problem.

Learn a function $y = f(x)$ from a given training set {${x_{1}, x_{2}, \dots x_{n}}$} ; {$y_{1}, y_{2}, \dots{y_{m}}$} so that $y_{i} \cong f(x_{i})$.

Each **observation** of $x$ is represented by **an** n-dimensional vector in **an** n-dimensional space.

For instance, a vector $x$ can be represented as **an** n-**element** vector $x_{i} = (x_{i_{1}}, x_{i_{2}}, \dots x_{i_{n}})^T$.

>[!note]
>$x^T$ is a **transposed** matrix. Normally, vectors are represented **horizontally**, so to represent it as a column **vector**, we use $x^T$.

---
## Linear model.

If $f(x)$ is assumed to be of linear form:

$$
f(x) = w_{0} + w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}
$$
	$w_{0}, w_{1}, \dots, w_{n}$ are the regression coefficients/weights.
	$w_{0}$ is sometimes called **bias**.

For each observation $x$, we have a corresponding **weight** $w$. If an observation **has a** higher $w$, it **indicates** that this vector $x$ (or feature $x$) is important.

In the linear function $f(x) = w_{0} + w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}$:

*   **$w_0$ is called the intercept term or bias term.**
*   It represents the value of $f(x)$ when all input variables $x_1, x_2, \dots, x_n$ are equal to 0.

Let's consider two cases:

1.  **When $w_0$ is present:**
    If you set all $x_i = 0$, we get:
    $f(0) = w_{0} + w_{1}(0) + w_{2}(0) +\dots + w_{n}(0)$
    $f(0) = w_{0}$

    This means that when all inputs are 0, the value of the function is $w_0$. If $w_0 \neq 0$, then the function will not pass through the origin $(0, 0, \dots, 0)$.

2.  **When $w_0$ is removed (i.e., $w_0 = 0$):**
    The function becomes:
    $f(x) = w_{1}x_{1}+w_{2}x_{2} +\dots + w_{n}x_{n}$

    Now, if you set all $x_i = 0$, we get:
    $f(0) = w_{1}(0)+w_{2}(0) +\dots + w_{n}(0)$
    $f(0) = 0$

    **Therefore, when $w_0$ is removed (or set to 0), the function $f(x)$ will always pass through the origin $(0, 0, \dots, 0)$** because when all inputs are 0, the output is also 0.

In a geometric context, $w_0$ determines the point where the line (or plane, hyperplane) intersects the $f(x)$ axis (or y-axis in 2D space) when all independent variables are zero. If $w_0=0$, that intersection point is precisely the origin.

>[!note]
>If a function can only pass through origin (0, 0,..., 0) it may not perform well so we add $w_{0}$.

>[!tip] Important
>Learning a linear model is equivalent to learning the coefficient vector $w = (w_{0}+ w_{1} + \dots + w_{n})^T$.

![[Pasted image 20250916124102.png]]

In **Linear Regression**, our missionn is to find the best line to represent all the data points.

---
## Prediction.

For each observation $x$ = $(x_{1}, x_{2}, \dots, x_{n})^T$:
- The true output: $c_{x}$ (Only in training data, future data - testing data won't learn its true output).

Prediction by our model:
$$
y_{x} = w_{0}+ w_{1}x_{1} +w_{2}x_{2} + \dots + w_{n}x_{n}
$$
Here,  are the individual features (or components) of the observation $x$.

>We often expect the $y_{x}\cong c_{x}$.

>[!note] Explanation:
>$y_{x}$ is the prediction value, we often expect the prediction value to approximately equal the true value $c_{x}$

After we have these weights, we can use this model to make a prediction on a future observation.

Future observation $z = (z_{1}, z_{2}, \dots, z_{n})^T$.

Use this learned function to make a prediction:

$$
f(z) = w_{0} + w_{1}z_{1} + w_{2}z_{2} + \dots + w_{n}z_{n}
$$

---
## But, how can we find this $f(x)$ function?

The main goal of all machine learning **algorithms** is to learn a function $f^*$ that can make the best predictions on unseen data (generalization).

![[Pasted image 20250916172401.png]]

>[!question]
>In this data space, there are **an infinite number of straight lines** we can define ($f(x)$), but how do we find the $f(x)$ function that **has** the best generalization?

**Difficulty**:
- How do we 'learn' - **which means finding the best  function?**
- How can we determine which function is better? - Is $f(x)$ better than $g(x)$?
=> Use a measure: **Loss function** - is often used to guide learning.

---
## Loss function.

**Definition**: 
- The **loss/error function** of the prediction for an observation $x = (x_{1}, x_{2}, \dots, x_{n})^T$ is:
$$
r(x) = [c_{x} - f(x)]^2 = [c_{x} - w_{0} - w_{1}x_{1} - w_{2}x_{2} - \dots - w_{n}x_{n}]^2
$$
- The **Expected loss** (kỳ vọng - average) of $f$ all over the whole space:
$$
E = E_{x}[r(x)] = E_{x}[c_{x} - f(x)]^2
$$
>$E_{x}$ is the expectation of $f$ over $x$.

So, we don't want error, right? $\to$ our goal is to find a $f^*$ function that have the min $E$.

$$
f^* = argmin_{f \in H}E_{x}[r(x)]
$$
>H is the space of all possible linear functions $f$.

The formula $E_{x}[r_{x}]$ represents average loss (expected loss) from true/real-world data distribution.

But this is **impossible** in real world problem, because we only have the **finite training dataset**, we don't have the whole dataset from all over the world.

---
## Empirical loss.

**Problem**: We can only observe a set of training data D = {$(x_{1}, y_{1}), (x_{2}, y_{2}), \dots (x_{M}, y_{M})$}, and we have to find $f$ from this $D$.

**Empirical loss** - Lỗi thực nghiệm, Residual Sum of Squares (**RSS**):
$$
RSS(f) = \sum_{i=1}^M(y_{i} - f(x_{i})^2 = \sum_{i=1}^M(y_{i} - w_{0} - w_{1}x_{i_{1}} - w_{2}x_{i_{2}} - \dots - w_{n}x_{i_{n}}) 
$$
>[!success] Annotate
>M represents all the observations we have in the training set, n represents all the elements we have from each observation $x$.

For instance, we have 10 data points => **M** = 10, and each data point $x$ has 5 elements (features) => **n** = 5.

And, to get the $E_{x}$ on $D$ set (Trung bình lỗi trên tập D), we basically use:
$$
\frac{1}{M}RSS(f) 
$$
This is $\cong$ $E_{x}[r(x)]$.

**Generalization error** (Impossible in real-world scenario).
$$
\frac{1}{M}RSS(f) - E_{x}[r(x)]
$$
