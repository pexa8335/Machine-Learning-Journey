---
tags:
  - LinearRegression
  - ML
  - "#RSS"
Date: 2025-09-17
Relevant:
  - "[[3.1 - Linear Regression.]]"
  - "[[Euclidean norm (L2 norm).]]"
  - "[[Mathematical proof - Normal Equation.]]"
Video: https://www.youtube.com/watch?v=fEYG6n26KjU
---
# Th[[3.2 - Ordinary Least Square (OLS).]]e OLS function.

>[!important]
>Finding the $f^*$ function in a linear space is equivalent to finding an optimized weight vector $w^*$ 

From the previous lesson [[3.1 - Linear Regression.]], we know that we can't find a loss function $E_{x}[r(x)]$on the whole $x$ space, so we use RSS on a specific training set $D$.

So, given the dataset $D$, our goal is to find $f^*$ that minimizes RSS:

$$
f^* = argmin_{f \in D}RSS(f)
$$
$$
\Leftrightarrow w^* = argmin_{w} \sum_{1}^M(y_{i} - w_{0} - w_{1}x_{i_{1}} - w_{2}x_{i_{2}} - \dots - w_{n}x_{i_{n}})^2
$$

>n represents the number of elements in each vector x, M is the number of vectors x [[3.1 - Linear Regression.]]

>[!important] Understanding $f^*$ and $w^*$
>In the context of Ordinary Least Squares (OLS) for linear regression:
>
>*   **$f^*$ (Optimal Prediction Function):** The ideal function that best maps the input features ($x$) to the output target ($y$).
>*   **$w^*$ (Optimal Weight Vector):** Since we are working within a *linear space*, the prediction function $f(x)$ is defined by a set of weights (coefficients) $w = [w_0, w_1, \dots, w_n]^T$. Therefore, finding the optimal function $f^*$ is **equivalent** to finding the optimal set of weights $w^*$ that define that function.
>
>In essence, $w^*$ are the parameters that define the optimal linear function $f^*$.

>[!note] 
>We need to place $w$ below *argmin* to specify which variables we want to optimize using the OLS function.
>$argmin_{w}$ means finding the value of $w$ that we minimizes the expression that follow.

This quadratic function, defined over the $(n+1)$-dimensional weight space, is convex (parabol) and always has at least one global minimum point. **Finding this point is the solution to the problem.**

Find $w^*$ by taking the gradient (deriative) of RSS and solving the equation
$$
RSS(w)' = 0
$$
We have **Normal Equation** formula.
$$
w^* = (X^TX)^{-1}X^Ty
$$
Why do we have this formula? Visit [[Mathematical proof - Normal Equation.]]

>[!warning]
>Assume $X^TX$ is invertible.

>[!important]
>$X^TX$ is always a square matrix.

Mathematical proof? Assume $X \in R^{m\times n} \implies X^T \in R^{n \times m}$
$$
X^TX = C^{(m \times n) \times (n \times m)} = C^{m \times m}
$$
---
## Methods: OLS.

Input: $D =$ {$(x_{1}, y_{1}), (x_{2}, y_{2}), \dots, (x_{M}, y_{M})$} 
Output: $w^*$.
Learning: compute
$$
w^* = (X^TX)^{-1}X^Ty
$$
Prediction for a new $x$.
$$
y_{x} = w^*_{0} + w^*_{1}x_{1} +\dots+w^*_{n}x_{n}
$$

![[Pasted image 20250917172354.png]]

It tends to **overfitting** because the learning phase just focus on minimizing the error.