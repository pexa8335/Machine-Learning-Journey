---
tags:
  - LinearRegression
  - Math
  - ML
  - RSS
  - "#Regularization"
Date: 2025-09-18
Video: https://www.youtube.com/watch?v=-ya_CVVr59w&list=PLaKukjQCR56ZRh2cAkweftiZCF2sTg11_&index=9
Relevant:
  - "[[3.1 - Linear Regression.]]"
  - "[[3.2 - Ordinary Least Square (OLS).]]"
  - "[[Euclidean norm (L2 norm).]]"
---
# 1. Review OLS.

In [[3.2 - Ordinary Least Square (OLS).]] and [[Mathematical proof - Normal Equation.]], our goal is to find the optimal set of weights $w^*$ to define the best function $f^*$.
We find $w^*$ by minimizing the Residual Sum of Squares (RSS) function, defined as:
$$
RSS(w) = ||y - Aw||^2
$$

However, the problem with OLS is **overfitting** in some specific cases. Therefore, adding regularization is a method to solve this problem.

**Problem**: Given the training dataset $D = \{(x_{1}, y_{1}), (x_{2}, y_{2}), \dots (x_{M}, y_{M})\}$, we need to find the function $f^*$ from this set $D$.

>[!success] Annotate
>M represents the total number of observations in the training set, while n represents the number of features for each observation $x$.

# Ridge Regression - L2 Regularization.

Ridge Regression addresses the overfitting problem by adding a **regularization term** to the OLS objective function. The objective function for Ridge Regression is:
$$
J(w) = ||y - Aw||^2 + \lambda||w||^2_{2}
$$
Where $\lambda \ge 0$ is the **regularization parameter.**
The subscript $_{2}$ denotes that this is the [[Euclidean norm (L2 norm).]].

The goal of Ridge Regression is to find $w^*$ by minimizing the function $J(w)$:
$$
w^* = argmin_{w} (||y-Aw||^2 + \lambda||w||^2_{2})
$$
Unlike [[3.2 - Ordinary Least Square (OLS).]], which can be prone to overfitting, with the parameter $\lambda$, Ridge Regression balances minimizing the prediction error ($||y-Aw||^2$) and keeping the weights $w$ small ($||w||^2_{2}$). This helps control the model's complexity and prevents **overfitting**.

>[!important]
>Ridge Regression creates a trade-off between *training error* and the model's *generalization ability* on new data.

A very large $\lambda$ causes the model to suffer from underfitting.

This is because a large $\lambda$ excessively penalizes the magnitude of weights ($w$), forcing them to become extremely small, often approaching zero. 

When weights are near zero, the model becomes too simplistic and cannot capture the underlying patterns in the data.

This leads to high training error and poor performance on both training and new data.

In extreme scenarios, the model may predict values close to zero for any input, rendering it ineffective.

---
## Methods.

We solve $w^*$:
$$
w^* = (A^TA + \lambda I_{n+1})^{-1}A^Ty
$$
>[!important] Property
>If $\lambda$ > 0, this equation is always invertible.

Prediction for a new $x$:
$$ 
y = w^*_{0} + w^*x_{1} + \dots + w^*_{n}x_{n}
$$

---
## Compared to OLS.

Avoid non-invertible matrix.

Reduce overfitting.

Training error may increase.

Need to find the best $\lambda$ - hyperparameter.

---
