---
tags:
  - LinearRegression
  - Math
  - ML
  - "#GradientDescent"
Date: 2025-09-24
Code:
Relevant:
  - "[[Lecture content/02 - Linear Regression & Gradient Descent/Linear Regression.|Linear Regression.]]"
---
>[!quote] Note
>This should be learned together with the code file.


## Motivation.

**BÃ i toÃ¡n Ä‘Æ¡n giáº£n thÃ¬ cÃ³ cÃ´ng thá»©c giáº£i, nhÆ°ng bÃ i toÃ¡n phá»©c táº¡p thÃ¬ khÃ´ng**

- Vá»›i linear regression 1 tham sá»‘ â†’ giáº£i báº±ng tay Ä‘Æ°á»£c. [[Lecture content/02 - Linear Regression & Gradient Descent/Linear Regression.|Linear Regression.]]
- Vá»›i linear regression nhiá»u tham sá»‘ â†’ váº«n cÃ³ cÃ´ng thá»©c (normal equation).
- NhÆ°ng khi chuyá»ƒn sang deep learning (máº¡ng nÆ¡-ron nhiá»u lá»›p, nhiá»u triá»‡u tham sá»‘) â†’ khÃ´ng cÃ²n cÃ´ng thá»©c giáº£i Ä‘Ã³ng, vÃ¬ hÃ m loss cá»±c ká»³ phá»©c táº¡p, khÃ´ng thá»ƒ giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m = 0.
â‡’ LÃºc nÃ y, **gradient descent lÃ  cÃ¡ch duy nháº¥t kháº£ thi** Ä‘á»ƒ tÃ¬m nghiá»‡m xáº¥p xá»‰.

## TrÆ°á»ng há»£p phá»©c táº¡p (multiple parameters)

NhÆ°ng vá»›iÂ **Linear Regression thá»±c táº¿**, model thÆ°á»ng cÃ³ dáº¡ng:  
**y = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™**

Khi Ä‘Ã³:
- CÃ³ n+1 tham sá»‘ cáº§n tá»‘i Æ°u: wâ‚€, wâ‚, wâ‚‚, ..., wâ‚™
- Pháº£i tÃ­nh Ä‘áº¡o hÃ m riÃªng theo tá»«ng tham sá»‘: âˆ‚L/âˆ‚wâ‚€ = 0, âˆ‚L/âˆ‚wâ‚ = 0, ...
- Äiá»u nÃ y táº¡o raÂ **há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh**Â vá»›i n+1 áº©n sá»‘
- Giáº£i há»‡ nÃ y báº±ng Ä‘áº¡i sá»‘ ma tráº­n dáº«n Ä‘áº¿n Normal Equation:Â **Î¸ = (X^T X)â»Â¹ X^T y**

Táº¡m thá»i chÆ°a Ä‘i sÃ¢u vÃ o khÃ¡i niá»‡m **Normal Equation**.

---
# Gradient Descent algorithm.

Input gá»“m: 
- HÃ m $f(x)$ cáº§n cá»±c tiá»ƒu hÃ³a.
- Initialized value $x_{0}$ táº¡i t = 0.
- Learning rate $\alpha > 0$.

while [end condition not satisfied] do:
- $g_{t}$ = $f'(x_{t})$
- $x_{t+1} = x_{t} - \alpha g_{t}$
- t = t + 1

From each iteration $t$, calculate Ä‘áº¡o hÃ m táº¡i $f(x_{t})$ vÃ  cáº­p nháº­t giÃ¡ trá»‹ x táº¡i iteration tiáº¿p theo ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m.

>[!question]
>Táº¡i sao pháº£i cáº­p nháº­t giÃ¡ trá»‹ $x_{t+1}$ ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m?
Báº¡n Ä‘ang nÃªu Ä‘Ãºng Ã½ tÆ°á»Ÿng chung, nhÆ°ng cÃ³ má»™t chÃºt nháº§m láº«n nhá» ğŸ‘‡

- fâ€²(xt)>0f'(x_t) > 0: Ä‘á»“ thá»‹ **Ä‘ang dá»‘c lÃªn** khi Ä‘i sang pháº£i â†’ muá»‘n giáº£m hÃ m sá»‘ thÃ¬ pháº£i Ä‘i **sang trÃ¡i** (ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m).
- fâ€²(xt)<0f'(x_t) < 0: Ä‘á»“ thá»‹ **Ä‘ang dá»‘c xuá»‘ng** khi Ä‘i sang pháº£i â†’ muá»‘n giáº£m hÃ m sá»‘ thÃ¬ pháº£i Ä‘i **sang pháº£i** (ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m).
 
---
## Dataset.

Given $D = \{(x^{(i)}, t^{(i)}) \}_{i=1}^N$:

> $x^{(i)}$ are the input features for the $i^{th}$ data point.
> $t^{(i)}$ is the target value for the $i^{th}$ data point.

Trong vÃ­ dá»¥ nÃ y, target values will contain noises $\varepsilon$ nhá».
$$
\varepsilon^{(i)} \sim N(0, \sigma^2)
$$
$$
t^{(i)} = f(x^{(i)}) + \varepsilon^{(i)}
$$
vá»›i $f(x) = 3x+2$ is an unknown target function.

**LÆ°u Ã½**: hÃ m $f$ nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ minh há»a, thá»±c táº¿, khÃ´ng thá»ƒ biáº¿t $f$ cÅ©ng nhÆ° epsilon cÅ©ng nhÆ° phÃ¢n phá»‘i cá»§a noises epsilon.

## Model.

Thá»±c nghiá»‡m vá»›i model Ä‘Æ¡n giáº£n:
$$
y = f_{w}(x) = w_{0}x
$$
vá»›i $w_{0}$ is the only one parameter of model.

## Loss function.

Using MSE loss function to evaluate model performance.
$$
L(w) = \frac{1}{N}\sum_{i=1}^N(t^{(i)} - f_{w}(x^{(i)}))^2 
$$
## Optimization.

Using **Gradient Descent algorithm** to find the best $w_{0}$ value so that it can cá»±c tiá»ƒu hÃ³a loss function MSE.

For each iteration $t$, we update:
$$
w_{0}^{(t+1)} = w_{0}^{(t)} - \alpha L'(w_{0}^{(t)})
$$
t Ä‘áº¡i diá»‡n cho thá»i Ä‘iá»ƒm/vÃ²ng láº·p thá»© t, $w_{0}$ á»Ÿ má»—i vÃ²ng láº·p sáº½ báº±ng $w_{0}$ á»Ÿ vÃ²ng láº·p trÆ°á»›c Ä‘Ã³ trá»« Ä‘i $\alpha$ nhÃ¢n vá»›i giÃ¡ trá»‹ Ä‘áº¡o hÃ m cá»§a L'(w) táº¡i vá»›i $w_{0}$ á»Ÿ thá»i Ä‘iá»ƒm $t$.


