---
tags:
  - LinearRegression
  - Math
  - ML
  - "#GradientDescent"
Date: 2025-09-24
Code: Gradient_Descent.ipynb
Relevant:
  - "[[Lecture content/02 - Linear Regression & Gradient Descent/Linear Regression.|Linear Regression.]]"
---
>[!quote] Note
>This should be learned together with the code file.


## Motivation.

**BÃ i toÃ¡n Ä‘Æ¡n giáº£n thÃ¬ cÃ³ cÃ´ng thá»©c giáº£i, nhÆ°ng bÃ i toÃ¡n phá»©c táº¡p thÃ¬ khÃ´ng**

- Vá»›i linear regression 1 tham sá»‘ â†’ giáº£i báº±ng tay Ä‘Æ°á»£c. [[Lecture content/02 - Linear Regression & Gradient Descent/Linear Regression.|Linear Regression.]]
- Vá»›i linear regression nhiá»u tham sá»‘ â†’ váº«n cÃ³ cÃ´ng thá»©c (normal equation).
- NhÆ°ng khi chuyá»ƒn sang deep learning (máº¡ng nÆ¡-ron nhiá»u lá»›p, nhiá»u triá»‡u tham sá»‘) â†’ khÃ´ng cÃ²n cÃ´ng thá»©c giáº£i Ä‘Ã³ng, vÃ¬ hÃ m loss cá»±c ká»³ phá»©c táº¡p, khÃ´ng thá»ƒ giáº£i phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m = 0.
â‡’ LÃºc nÃ y, **gradient descent lÃ  cÃ¡ch duy nháº¥t kháº£ thi** Ä‘á»ƒ tÃ¬m nghiá»‡m xáº¥p xá»‰.

## TrÆ°á»ng há»£p phá»©c táº¡p (multiple parameters)

NhÆ°ng vá»›iÂ **Linear Regression thá»±c táº¿**, model thÆ°á»ng cÃ³ dáº¡ng:  
**y = wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™**

Khi Ä‘Ã³:
- CÃ³ n+1 tham sá»‘ cáº§n tá»‘i Æ°u: wâ‚€, wâ‚, wâ‚‚, ..., wâ‚™
- Pháº£i tÃ­nh Ä‘áº¡o hÃ m riÃªng theo tá»«ng tham sá»‘: âˆ‚L/âˆ‚wâ‚€ = 0, âˆ‚L/âˆ‚wâ‚ = 0, ...
- Äiá»u nÃ y táº¡o raÂ **há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh**Â vá»›i n+1 áº©n sá»‘
- Giáº£i há»‡ nÃ y báº±ng Ä‘áº¡i sá»‘ ma tráº­n dáº«n Ä‘áº¿n Normal Equation:Â **Î¸ = (X^T X)â»Â¹ X^T y**

Táº¡m thá»i chÆ°a Ä‘i sÃ¢u vÃ o khÃ¡i niá»‡m **Normal Equation**.

---
# 1. Gradient Descent algorithm.

**Input:** 
- HÃ m $f(x)$ cáº§n cá»±c tiá»ƒu hÃ³a.
- Initialized value $x_{0}$ táº¡i t = 0.
- Learning rate $\alpha > 0$.

while [end condition not satisfied] do:
- $g_{t}$ = $f'(x_{t})$
- $x_{t+1} = x_{t} - \alpha g_{t}$
- t = t + 1

From each iteration $t$, calculate Ä‘áº¡o hÃ m táº¡i $f(x_{t})$ vÃ  cáº­p nháº­t giÃ¡ trá»‹ x táº¡i iteration tiáº¿p theo ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m.

>[!question]
>Táº¡i sao pháº£i cáº­p nháº­t giÃ¡ trá»‹ $x_{t+1}$ ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m?
Báº¡n Ä‘ang nÃªu Ä‘Ãºng Ã½ tÆ°á»Ÿng chung, nhÆ°ng cÃ³ má»™t chÃºt nháº§m láº«n nhá» ğŸ‘‡

- fâ€²(xt)>0f'(x_t) > 0: Ä‘á»“ thá»‹ **Ä‘ang dá»‘c lÃªn** khi Ä‘i sang pháº£i â†’ muá»‘n giáº£m hÃ m sá»‘ thÃ¬ pháº£i Ä‘i **sang trÃ¡i** (ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m).
- fâ€²(xt)<0f'(x_t) < 0: Ä‘á»“ thá»‹ **Ä‘ang dá»‘c xuá»‘ng** khi Ä‘i sang pháº£i â†’ muá»‘n giáº£m hÃ m sá»‘ thÃ¬ pháº£i Ä‘i **sang pháº£i** (ngÆ°á»£c chiá»u Ä‘áº¡o hÃ m).

>[!important]
>- Learning rate shouldn't be too large vÃ¬ bÆ°á»›c nháº£y cá»§a nÃ³ sáº½ cÃ³ thá»ƒ bá» qua Ä‘iá»ƒm cá»±c trá»‹.
>- Learning rate shouldn't be too small vÃ¬ sáº½ tá»‘n nhiá»u thá»i gian training.

---
# 2. Gradient Descent in our problem.

**Input**:
- HÃ m máº¥t mÃ¡t $L(w) = MSE(w_{0})$ cáº§n minimize.
- Initialized value $w_{0}$ táº¡i t = 0.
- Learning rate $\alpha > 0$.
while [condition not satisfied] do:
- $g_{t} = L'(w_{0})$
- $w_{0_{t+1}} = w_{0}-\alpha.g_{t}$
- t = t + 1
We want to find the value of $w_{0}$ that can minimize the loss function MSE(w).

$$
w_{0}^* = argmin_{w_{0}}L(w) = argmin_{w_{0}} \frac{1}{N} \sum_{i=1}^N(t^{(i)} - w_{0}x^{(i)})^2
$$
Äáº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t MSE theo $w_{0}$ lÃ :

$$
\frac{dL}{dw_{0}} = \frac{1}{N}\sum_{i=1}^N 2(w_{0}x^{(i)}-t^{(i)})x^{(i)}
$$
---
## 2.1 Dataset.

Given $D = \{(x^{(i)}, t^{(i)}) \}_{i=1}^N$:

> $x^{(i)}$ are the input features for the $i^{th}$ data point.
> $t^{(i)}$ is the target value for the $i^{th}$ data point.

Trong vÃ­ dá»¥ nÃ y, target values will contain noises $\varepsilon$ nhá».
$$
\varepsilon^{(i)} \sim N(0, \sigma^2)
$$
$$
t^{(i)} = f(x^{(i)}) + \varepsilon^{(i)}
$$
vá»›i $f(x) = 3x+2$ is an unknown target function.

**LÆ°u Ã½**: hÃ m $f$ nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ minh há»a, thá»±c táº¿, khÃ´ng thá»ƒ biáº¿t $f$ cÅ©ng nhÆ° epsilon cÅ©ng nhÆ° phÃ¢n phá»‘i cá»§a noises epsilon.

## 2.2 Model.

Thá»±c nghiá»‡m vá»›i model Ä‘Æ¡n giáº£n:
$$
y = f_{w}(x) = w_{0}x
$$
vá»›i $w_{0}$ is the only one parameter of model.

## 2.3 Loss function.

Using MSE loss function to evaluate model performance.
$$
L(w) = \frac{1}{N}\sum_{i=1}^N(t^{(i)} - f_{w}(x^{(i)}))^2 
$$
## 2.4 Optimization.

Using **Gradient Descent algorithm** to find the best $w_{0}$ value so that it can cá»±c tiá»ƒu hÃ³a loss function MSE.

For each iteration $t$, we update:
$$
w_{0}^{(t+1)} = w_{0}^{(t)} - \alpha L'(w_{0}^{(t)})
$$
t Ä‘áº¡i diá»‡n cho thá»i Ä‘iá»ƒm/vÃ²ng láº·p thá»© t, $w_{0}$ á»Ÿ má»—i vÃ²ng láº·p sáº½ báº±ng $w_{0}$ á»Ÿ vÃ²ng láº·p trÆ°á»›c Ä‘Ã³ trá»« Ä‘i $\alpha$ nhÃ¢n vá»›i giÃ¡ trá»‹ Ä‘áº¡o hÃ m cá»§a L'(w) táº¡i vá»›i $w_{0}$ á»Ÿ thá»i Ä‘iá»ƒm $t$.

![[Pasted image 20250926235137.png]]

NhÆ°ng, váº¥n Ä‘á» cá»§a hÃ m 1 biáº¿n $w_{0}x$ lÃ  luÃ´n Ä‘i qua gá»‘c tá»a Ä‘á»™, vá»›i sample $x^{(i)}$ nhÆ° váº­y, sáº½ khÃ´ng cÃ³ Ä‘Æ°á»ng tháº³ng nÃ o Ä‘i qua gá»‘c tá»a Ä‘á»™ tháº­t sá»± tá»‘t Ä‘á»ƒ biá»ƒu diá»…n toÃ n bá»™ dataset.

---
## 2.5 Model.

Ta thá»±c nghiá»‡m vá»›i má»™t model má»›i.
$$
y = f_{w}(x) = w_{0} + w_{1}x
$$
Vá»›i $w = (w_{0}, w_{1})$ lÃ  bá»™ tham sá»‘ cáº§n tá»‘i Æ°u hÃ³a cá»§a model.

## 2.6 Loss function.

HÃ m máº¥t mÃ¡t MSE Ä‘á»ƒ evaluate model's performance sáº½ cÃ³ thay Ä‘á»•i:
$$
L(w) = \frac{1}{N}\sum_{i=1}^N(t^{(i)} - wx^{(i)})^2
$$
$$
= \frac{1}{N}\sum_{i=1}^N(t^{(i)} - (w_{0} + w_{1}x^{(i)}))^2
$$
## 2.7 Optimization.

Sá»­ dá»¥ng Gradient Descent Ä‘á»ƒ tÃ¬m ra táº­p tham sá»‘ $w = (w_{0}, w_{1})$ sao cho cÃ³ thá»ƒ minimize loss function MSE.

Calculate partial deriative of this loss function theo $w_{0}$ vÃ  $w_{1}$
$$
\frac{\partial L(\mathbf{w})}{\partial w_0} = \frac{1}{N}\sum_{i=1}^N \frac{\partial}{\partial w_{0}}(t^{(i)} - (w_{0} + w_{1}x^{(i)}))^2
$$
$$
= \frac{1}{N}\sum_{i=1}^N 2((w_{0} +w_{1}x^{(i)}) - t^{(i)})
$$

$$
 \frac{\partial L(\mathbf{w})}{\partial w_{1}} = \frac{1}{N} \sum_{i=1}^N 2((w_{0} + w_{1}x^{(i)})-t^{(i)})x^{(i)}
$$

# Gradient.

Äá»‹nh nghÄ©a **gradient $\nabla_{w}L(w)$** cá»§a loss function MSE $L(w)$ vá»›i vector thÃ nh pháº§n lÃ  cÃ¡c Ä‘áº¡o hÃ m riÃªng cá»§a $L(w)$ theo tá»«ng tham sá»‘ cá»§a bá»™ tham sá»‘ $w = (w_{0}, w_{1})$.

$$
\nabla_w L(\mathbf{w}) = \begin{bmatrix} \frac{\partial L(\mathbf{w})}{\partial w_0} \\ \frac{\partial L(\mathbf{w})}{\partial w_1} \end{bmatrix} = \frac{2}{N} \begin{bmatrix} \sum_{i=1}^N ((w_0 + w_1 x^{(i)}) - t^{(i)}) \cdot 1 \\ \sum_{i=1}^N ((w_0 + w_1 x^{(i)}) - t^{(i)}) \cdot x^{(i)} \end{bmatrix} = \frac{2}{N} \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{t})
$$
KÃ½ hiá»‡u:
$$
\mathbf{X} = \begin{bmatrix} 1 & x^{(1)} \\ 1 & x^{(2)} \\ \vdots & \vdots \\ 1 & x^{(N)} \end{bmatrix}, \quad \mathbf{w} = \begin{bmatrix} w_0 \\ w_1 \end{bmatrix}, \quad \mathbf{t} = \begin{bmatrix} t^{(1)} \\ t^{(2)} \\ \vdots \\ t^{(N)} \end{bmatrix}
$$

>[!question]
>Táº¡i sao cÃ³ cá»™t 1 á»Ÿ vector X?
>- Äáº·t nhÃ¢n tá»­ chung, $w_{1}x^{(i)}$ thÃ¬ rÃºt $x^{(i)}$, $w_{0} \Leftrightarrow w_{0}\times_{1}$ nÃªn rÃºt 1. 

>[!question]
>Táº¡i sao láº¡i cÃ³ $X^T$ á»Ÿ Ä‘Ã¢y thay vÃ¬ $x^{(i)}$?

$\mathbf{X}: n \times 2$

$\mathbf{w}: 2 \times 1$

$\mathbf{X}\mathbf{w} - \mathbf{t} = \mathbf{e}: n \times 1$

Náº¿u thá»­ $\mathbf{X} \times \mathbf{e}$:

$\mathbf{X}$ lÃ  $(n \times 2)$, $\mathbf{e}$ lÃ  $(n \times 1)$ $\rightarrow$ khÃ´ng nhÃ¢n Ä‘Æ°á»£c (vÃ¬ inner dimension $2 \neq n$).

CÃ²n $\mathbf{X}^T \mathbf{e}$:

$\mathbf{X}^T$ lÃ  $(2 \times n)$, $\mathbf{e}$ lÃ  $(n \times 1)$ $\rightarrow$ káº¿t quáº£ $(2 \times 1)$, cÃ¹ng shape vá»›i $\mathbf{w}$.

>[!danger] Important
>Gradient $\nabla_{w}L(w)$ lÃ  vector trá» vá» hÆ°á»›ng giÃ¡ trá»‹ L(w) tÄƒng, ta Ä‘ang cáº§n minimize hÃ m máº¥t mÃ¡t nÃªn ta pháº£i cáº­p nháº­t giÃ¡ trá»‹ bá»™ tham sá»‘ $w = (w_{0}, w_{1})$ theo hÆ°á»›ng ngÆ°á»£c láº¡i vá»›i Ä‘áº¡o hÃ m (nhÆ° Ä‘áº§u bÃ i Ä‘Ã£ nÃ³i lÃ­ do): $-\nabla_{w}L(w)$


Äáº§u vÃ o: 
- HÃ m máº¥t mÃ¡t $L(\mathbf{w})=\text{MSE}(w_0)$ cáº§n cá»±c tiá»ƒu hÃ³a.
- GiÃ¡ trá»‹ khá»Ÿi táº¡o $\mathbf{w}$ táº¡i $t=0$ (initial value).
- Há»‡ sá»‘ há»c (learning rate) $\alpha > 0$.
while [Ä‘iá»u kiá»‡n dá»«ng chÆ°a thá»a] do:
		$\mathbf{g}_t = \nabla_w L(\mathbf{w})$
		$\mathbf{w} = \mathbf{w} - \alpha \mathbf{g}_t$
		$t = t+1$
